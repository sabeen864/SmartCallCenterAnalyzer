{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a424286-5a38-40dc-bd16-226d331eff04",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Advanced Feature Engineering for Customer Churn\n",
    "\n",
    "In this notebook, we engineer additional features from the cleaned dataset:\n",
    "- üìÑ Textual features (lengths, sentiment)\n",
    "- üî§ TF‚ÄëIDF embeddings\n",
    "- ‚è∞ Time‚Äëbased features\n",
    "- üë§ Interaction features\n",
    "\n",
    "The goal is to create a rich feature set that improves our churn prediction model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29ae95-1e47-46fc-8edd-4ac7ce843fe5",
   "metadata": {},
   "source": [
    "## üì• Load Cleaned Data\n",
    "We start with the cleaned dataset prepared in the previous step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accc5365-6a46-4aa5-aae8-116c2c9f0129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Loaded\n",
      "   tweet_id   author_id  inbound                 created_at  \\\n",
      "0         1  sprintcare    False  2017-10-31 22:10:47+00:00   \n",
      "1         4  sprintcare    False  2017-10-31 21:54:49+00:00   \n",
      "2         6  sprintcare    False  2017-10-31 21:46:24+00:00   \n",
      "3        11  sprintcare    False  2017-10-31 22:10:35+00:00   \n",
      "4        15  sprintcare    False  2017-10-31 20:03:31+00:00   \n",
      "\n",
      "                                                text response_tweet_id  \\\n",
      "0  @115712 I understand. I would like to assist y...                 2   \n",
      "1  @115712 Please send us a Private Message so th...                 3   \n",
      "2  @115712 Can you please send us a private messa...               5,7   \n",
      "3  @115713 This is saddening to hear. Please shoo...               NaN   \n",
      "4  @115713 We understand your concerns and we'd l...                12   \n",
      "\n",
      "   customer_tweet_id        customer_created_at  \\\n",
      "0                3.0  2017-10-31 22:08:27+00:00   \n",
      "1                5.0  2017-10-31 21:49:35+00:00   \n",
      "2                8.0  2017-10-31 21:45:10+00:00   \n",
      "3               12.0  2017-10-31 22:04:47+00:00   \n",
      "4               16.0  2017-10-31 20:00:43+00:00   \n",
      "\n",
      "                                       customer_text  response_time  ...  \\\n",
      "0  @sprintcare I have sent several private messag...       2.333333  ...   \n",
      "1                                 @sprintcare I did.       5.233333  ...   \n",
      "2          @sprintcare is the worst customer service       1.233333  ...   \n",
      "3  @sprintcare You gonna magically change your co...       5.800000  ...   \n",
      "4  @sprintcare Since I signed up with you....Sinc...       2.800000  ...   \n",
      "\n",
      "                                        cleaned_text char_count  word_count  \\\n",
      "0  i understand i would like to assist you we wou...        110          22   \n",
      "1  please send us a private message so that we ca...        112          22   \n",
      "2  can you please send us a private message so th...         94          18   \n",
      "3  this is saddening to hear please shoot us a dm...         87          20   \n",
      "4  we understand your concerns and wed like for y...        120          24   \n",
      "\n",
      "   avg_word_len  sentiment  hour  day_of_week  is_weekend  author_tweet_count  \\\n",
      "0      5.000000     0.6369    22            1           0               22209   \n",
      "1      5.090909     0.4767    21            1           0               22209   \n",
      "2      5.222222     0.7152    21            1           0               22209   \n",
      "3      4.350000    -0.5106    22            1           0               22209   \n",
      "4      5.000000     0.5859    20            1           0               22209   \n",
      "\n",
      "   author_mean_response_time  \n",
      "0                  61.400065  \n",
      "1                  61.400065  \n",
      "2                  61.400065  \n",
      "3                  61.400065  \n",
      "4                  61.400065  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1261888 entries, 0 to 1261887\n",
      "Data columns (total 21 columns):\n",
      " #   Column                     Non-Null Count    Dtype  \n",
      "---  ------                     --------------    -----  \n",
      " 0   tweet_id                   1261888 non-null  int64  \n",
      " 1   author_id                  1261888 non-null  object \n",
      " 2   inbound                    1261888 non-null  bool   \n",
      " 3   created_at                 1261888 non-null  object \n",
      " 4   text                       1261888 non-null  object \n",
      " 5   response_tweet_id          457153 non-null   object \n",
      " 6   customer_tweet_id          1261888 non-null  float64\n",
      " 7   customer_created_at        1261888 non-null  object \n",
      " 8   customer_text              1261888 non-null  object \n",
      " 9   response_time              1261888 non-null  float64\n",
      " 10  churn_label                1261888 non-null  int64  \n",
      " 11  cleaned_text               1259381 non-null  object \n",
      " 12  char_count                 1261888 non-null  int64  \n",
      " 13  word_count                 1261888 non-null  int64  \n",
      " 14  avg_word_len               1261888 non-null  float64\n",
      " 15  sentiment                  1261888 non-null  float64\n",
      " 16  hour                       1261888 non-null  int64  \n",
      " 17  day_of_week                1261888 non-null  int64  \n",
      " 18  is_weekend                 1261888 non-null  int64  \n",
      " 19  author_tweet_count         1261888 non-null  int64  \n",
      " 20  author_mean_response_time  1261888 non-null  float64\n",
      "dtypes: bool(1), float64(5), int64(8), object(7)\n",
      "memory usage: 193.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load cleaned data from Step 1\n",
    "df = pd.read_csv(\"../data/processed/cleaned_twcs.csv\")\n",
    "\n",
    "print(\"‚úÖ Data Loaded\")\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca2d96a-98b1-4643-b195-3bc1b19b9834",
   "metadata": {},
   "source": [
    "## ‚ú® Textual Features\n",
    "We extract simple yet powerful features from the `cleaned_text` column:\n",
    "- Text length (characters, words)\n",
    "- Average word length\n",
    "- Sentiment polarity (using VADER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ababc21-b5ef-46b0-a4b3-ae644448ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure cleaned_text is string and handle missing values\n",
    "df['cleaned_text'] = df['cleaned_text'].fillna('').astype(str)\n",
    "\n",
    "# Character count\n",
    "df['char_count'] = df['cleaned_text'].apply(len)\n",
    "\n",
    "# Word count\n",
    "df['word_count'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Average word length\n",
    "df['avg_word_len'] = df['char_count'] / df['word_count'].replace(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0682ae5-8e11-437d-9fb0-4c2b4ee66f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentiment scores added.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i understand i would like to assist you we wou...</td>\n",
       "      <td>0.6369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please send us a private message so that we ca...</td>\n",
       "      <td>0.4767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>can you please send us a private message so th...</td>\n",
       "      <td>0.7152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is saddening to hear please shoot us a dm...</td>\n",
       "      <td>-0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we understand your concerns and wed like for y...</td>\n",
       "      <td>0.5859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  sentiment\n",
       "0  i understand i would like to assist you we wou...     0.6369\n",
       "1  please send us a private message so that we ca...     0.4767\n",
       "2  can you please send us a private message so th...     0.7152\n",
       "3  this is saddening to hear please shoot us a dm...    -0.5106\n",
       "4  we understand your concerns and wed like for y...     0.5859"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df['sentiment'] = df['cleaned_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "print(\"‚úÖ Sentiment scores added.\")\n",
    "df[['cleaned_text','sentiment']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e155b-6899-431b-ae0b-3c17284d1b3b",
   "metadata": {},
   "source": [
    "## üïí Time-Based Features\n",
    "From `created_at`, extract:\n",
    "- Hour of day\n",
    "- Day of week\n",
    "- Weekend flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c194018-4d11-4faa-8cf5-99a3e45709d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Time features created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-31 22:10:47+00:00</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-31 21:54:49+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-31 21:46:24+00:00</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-31 22:10:35+00:00</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-31 20:03:31+00:00</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  hour  day_of_week  is_weekend\n",
       "0 2017-10-31 22:10:47+00:00    22            1           0\n",
       "1 2017-10-31 21:54:49+00:00    21            1           0\n",
       "2 2017-10-31 21:46:24+00:00    21            1           0\n",
       "3 2017-10-31 22:10:35+00:00    22            1           0\n",
       "4 2017-10-31 20:03:31+00:00    20            1           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce', utc=True)\n",
    "\n",
    "# Hour of day\n",
    "df['hour'] = df['created_at'].dt.hour\n",
    "\n",
    "# Day of week (0=Monday, 6=Sunday)\n",
    "df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "\n",
    "# Weekend flag\n",
    "df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "print(\"‚úÖ Time features created.\")\n",
    "df[['created_at','hour','day_of_week','is_weekend']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674bc00-94dc-468f-be23-f49d7b4de51b",
   "metadata": {},
   "source": [
    "## üë§ Interaction Features\n",
    "Aggregate customer-level information:\n",
    "- Number of tweets per author\n",
    "- Mean response time per author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06647f28-b334-4a64-93e0-720cd9814d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interaction features created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_tweet_count</th>\n",
       "      <th>author_mean_response_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sprintcare</td>\n",
       "      <td>22209</td>\n",
       "      <td>61.400065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sprintcare</td>\n",
       "      <td>22209</td>\n",
       "      <td>61.400065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sprintcare</td>\n",
       "      <td>22209</td>\n",
       "      <td>61.400065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sprintcare</td>\n",
       "      <td>22209</td>\n",
       "      <td>61.400065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sprintcare</td>\n",
       "      <td>22209</td>\n",
       "      <td>61.400065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id  author_tweet_count  author_mean_response_time\n",
       "0  sprintcare               22209                  61.400065\n",
       "1  sprintcare               22209                  61.400065\n",
       "2  sprintcare               22209                  61.400065\n",
       "3  sprintcare               22209                  61.400065\n",
       "4  sprintcare               22209                  61.400065"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of tweets by author\n",
    "author_counts = df['author_id'].value_counts().to_dict()\n",
    "df['author_tweet_count'] = df['author_id'].map(author_counts)\n",
    "\n",
    "# Mean response time by author\n",
    "author_mean_resp = df.groupby('author_id')['response_time'].mean().to_dict()\n",
    "df['author_mean_response_time'] = df['author_id'].map(author_mean_resp)\n",
    "\n",
    "print(\"‚úÖ Interaction features created.\")\n",
    "df[['author_id','author_tweet_count','author_mean_response_time']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f0244-8868-408e-ba56-c7cb7d8da217",
   "metadata": {},
   "source": [
    "## üî§ TF‚ÄëIDF Features\n",
    "We vectorize the `cleaned_text` column to capture word importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76286b63-c57f-4461-8b97-fd68e94f7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TF‚ÄëIDF matrix shape: (1261888, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000, # keep top 5000 features\n",
    "    ngram_range=(1,2), # unigrams and bigrams\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(df['cleaned_text'])\n",
    "print(\"‚úÖ TF‚ÄëIDF matrix shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a74afd-6b98-4b47-bbfd-2af1b37de470",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Combine All Features\n",
    "We concatenate:\n",
    "- Engineered numerical features\n",
    "- TF‚ÄëIDF sparse matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0affc042-5b1c-4118-9068-3be2114073fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final feature matrix shape: (1261888, 5010)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "num_features = df[['char_count','word_count','avg_word_len','sentiment',\n",
    "                   'hour','day_of_week','is_weekend',\n",
    "                   'response_time','author_tweet_count','author_mean_response_time']].fillna(0)\n",
    "\n",
    "X_final = hstack([X_tfidf, num_features.values])\n",
    "\n",
    "print(\"‚úÖ Final feature matrix shape:\", X_final.shape)\n",
    "\n",
    "# Target variable\n",
    "y = df['churn_label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d984a00-693f-4e0a-9ad5-7a1a31f70743",
   "metadata": {},
   "source": [
    "## üíæ Save Engineered Features\n",
    "Save the sparse matrix and target for modeling step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6be7b9-dd36-40cd-a443-65ae9d703485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved TWCS with all features to '../data/processed/cleaned_twcs.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save TWCS with all features\n",
    "df.to_csv('../data/processed/cleaned_twcs.csv', index=False)\n",
    "print(\"‚úÖ Saved TWCS with all features to '../data/processed/cleaned_twcs.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0a7b0fd-df77-40bb-ba81-b5679b28a7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features and labels saved in data/features/ folder.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save features and labels\n",
    "joblib.dump(X_final, \"../data/features/X_features.pkl\")\n",
    "joblib.dump(y, \"../data/features/y_labels.pkl\")\n",
    "\n",
    "print(\"‚úÖ Features and labels saved in data/features/ folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ab3b3-be02-4979-9df8-96597923ee84",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Proceed to the modeling notebook:\n",
    "- Load `X_features.pkl` and `y_labels.pkl`\n",
    "- Train multiple models (Logistic Regression, Random Forest, XGBoost)\n",
    "- Perform hyperparameter tuning and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de2c125c-70e1-4ba0-a206-77c97275f2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(tfidf, \"../models/tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2ef11-ef25-42db-b804-e45dda8fd398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
