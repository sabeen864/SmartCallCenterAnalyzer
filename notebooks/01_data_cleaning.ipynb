{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2287323-42f9-4513-b669-1164300336ef",
   "metadata": {},
   "source": [
    "# 📞 Smart Call Center Analyzer  \n",
    "\n",
    "**🎯 Purpose:**  \n",
    "Build a project that predicts customer churn, tags sentiment, intents and priortization score from call‑like customer–agent interactions.\n",
    "\n",
    "**🏢 Business Value:**  \n",
    "Helps call centers to:  \n",
    "✅ Identify at‑risk customers (reduce churn)  \n",
    "✅ Track sentiment in real time (improve satisfaction)  \n",
    "✅ Intent Classification: T5-Small with zero-shot learning and regex fallbacks for accurate prioritization.\n",
    "\n",
    "**🛠️ Tech Stack:**  \n",
    "- Python (Pandas, scikit‑learn, Matplotlib/Seaborn)  \n",
    "- Hugging Face Transformers for GenAI  \n",
    "- Jupyter Notebook for documentation & reproducibility\n",
    "\n",
    "**📂 Datasets Used:**  \n",
    "- **Kaggle TWCS** – Customer Support on Twitter dataset  \n",
    "- **UCI Sentiment Labelled Sentences** – for validating sentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab812b-f0b3-4e58-ae7f-54843d488ccd",
   "metadata": {},
   "source": [
    "## 🧩 Step 1: Environment Setup  \n",
    "\n",
    "Install and import the minimal libraries we need:  \n",
    "\n",
    "- **Pandas / NumPy** → data manipulation  \n",
    "- **scikit‑learn** → churn model (logistic regression)  \n",
    "- **Matplotlib / Seaborn** → visualizations for stakeholders  \n",
    "- **Transformers** → for GenAI sentiment/summarization  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfd2a71b-72b4-4f34-918c-0a2aa5056eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"✅ Environment ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483b6df-ebe5-4a47-bd49-f3373bbbd5d8",
   "metadata": {},
   "source": [
    "## 📥 Step 2: Load Datasets  \n",
    "\n",
    "We are using **two real‑world open datasets**:  \n",
    "\n",
    "1. **TWCS (Twitter Customer Support)**  \n",
    "   - Mimics customer–agent interactions (like call center chats)  \n",
    "   - We'll derive features like `response_time` and `churn_label`\n",
    "\n",
    "2. **UCI Sentiment Labelled Sentences**  \n",
    "   - Contains pre‑labeled sentences (positive/negative)  \n",
    "   - Used to validate or fine‑tune our sentiment tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31b81ca9-b70a-4d4f-ad5e-6f771f201e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 TWCS shape: (2811774, 7)\n",
      "   tweet_id   author_id  inbound                      created_at  \\\n",
      "0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n",
      "1         2      115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
      "2         3      115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
      "\n",
      "                                                text response_tweet_id  \\\n",
      "0  @115712 I understand. I would like to assist y...                 2   \n",
      "1      @sprintcare and how do you propose we do that               NaN   \n",
      "2  @sprintcare I have sent several private messag...                 1   \n",
      "\n",
      "   in_response_to_tweet_id  \n",
      "0                      3.0  \n",
      "1                      1.0  \n",
      "2                      4.0  \n",
      "\n",
      "🔹 UCI shape: (1000, 2)\n",
      "                                                text  label\n",
      "0  So there is no way for me to plug it in here i...      0\n",
      "1                        Good case, Excellent value.      1\n",
      "2                             Great for the jawbone.      1\n"
     ]
    }
   ],
   "source": [
    "# Load TWCS raw dataset\n",
    "df_twcs = pd.read_csv('../data/raw/twcs.csv', encoding='utf-8')\n",
    "print(\"🔹 TWCS shape:\", df_twcs.shape)\n",
    "print(df_twcs.head(3))\n",
    "\n",
    "# Load UCI dataset\n",
    "df_uci = pd.read_csv(\n",
    "    '../data/raw/amazon_cells_labelled.txt',\n",
    "    sep='\\t', header=None,\n",
    "    names=['text', 'label']\n",
    ")\n",
    "print(\"\\n🔹 UCI shape:\", df_uci.shape)\n",
    "print(df_uci.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350e58b-52c7-4960-b485-f9947b1516de",
   "metadata": {},
   "source": [
    "## 🔎 Step 3: Quick Data Understanding  \n",
    "\n",
    "Before cleaning, **inspect the datasets** to understand their columns and potential issues.\n",
    "\n",
    "- For TWCS, we expect columns like `tweet_id`, `text`, `inbound`, `in_response_to_tweet_id`, `created_at`.  \n",
    "- For UCI, we have simple `text` and `label` columns.  \n",
    "\n",
    "👉 **Goal:** Identify missing values, malformed timestamps, or irrelevant fields early.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adb7e7d7-77a0-4bb5-b328-3243afc217ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWCS columns: ['tweet_id', 'author_id', 'inbound', 'created_at', 'text', 'response_tweet_id', 'in_response_to_tweet_id']\n",
      "\n",
      "Missing values in TWCS:\n",
      " response_tweet_id          1040629\n",
      "in_response_to_tweet_id     794335\n",
      "tweet_id                         0\n",
      "inbound                          0\n",
      "author_id                        0\n",
      "text                             0\n",
      "created_at                       0\n",
      "dtype: int64\n",
      "\n",
      "UCI columns: ['text', 'label']\n",
      "\n",
      "Missing values in UCI:\n",
      " text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"TWCS columns:\", df_twcs.columns.tolist())\n",
    "print(\"\\nMissing values in TWCS:\\n\", df_twcs.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nUCI columns:\", df_uci.columns.tolist())\n",
    "print(\"\\nMissing values in UCI:\\n\", df_uci.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4644dbf-8db9-4796-8577-161e4ad99830",
   "metadata": {},
   "source": [
    "## 🧹 Step 4: Data Cleaning & Preprocessing  \n",
    "\n",
    "We will clean and engineer features from the TWCS dataset:\n",
    "\n",
    "**🔧 Operations on TWCS**\n",
    "- ✅ Ensure `tweet_id` and `in_response_to_tweet_id` are numeric\n",
    "- ✅ Parse `created_at` to proper datetime\n",
    "- ✅ Separate **customer tweets** (`inbound=True`) from **agent replies** (`inbound=False`)\n",
    "- ✅ Merge them to calculate `response_time` in minutes\n",
    "- ✅ Remove invalid rows (negative or missing response times)\n",
    "- ✅ Deduplicate by `tweet_id`\n",
    "- ✅ Create a `churn_label` column by scanning text for churn‑related keywords\n",
    "- ✅ Clean text by removing URLs, mentions, and special characters\n",
    "\n",
    "**🔧 Operations on UCI**\n",
    "- ✅ Clean text in the same way for consistency\n",
    "- ✅ Labels are already provided (0 = negative, 1 = positive)\n",
    "\n",
    "👉 **Why?**  \n",
    "High‑quality cleaned features (like response_time and churn signals) are the foundation for accurate modeling and insight generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cef3346c-3aec-42fa-9088-b285cd0a3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned TWCS pairs shape: (1261888, 12)\n",
      "                                                text  \\\n",
      "0  @115712 I understand. I would like to assist y...   \n",
      "1  @115712 Please send us a Private Message so th...   \n",
      "2  @115712 Can you please send us a private messa...   \n",
      "3  @115713 This is saddening to hear. Please shoo...   \n",
      "4  @115713 We understand your concerns and we'd l...   \n",
      "\n",
      "                                        cleaned_text  response_time  \\\n",
      "0  i understand i would like to assist you we wou...       2.333333   \n",
      "1  please send us a private message so that we ca...       5.233333   \n",
      "2  can you please send us a private message so th...       1.233333   \n",
      "3  this is saddening to hear please shoot us a dm...       5.800000   \n",
      "4  we understand your concerns and wed like for y...       2.800000   \n",
      "\n",
      "   churn_label  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "\n",
      "✅ Cleaned UCI preview:\n",
      "                                                text  label  \\\n",
      "0  So there is no way for me to plug it in here i...      0   \n",
      "1                        Good case, Excellent value.      1   \n",
      "2                             Great for the jawbone.      1   \n",
      "3  Tied to charger for conversations lasting more...      0   \n",
      "4                                  The mic is great.      1   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  so there is no way for me to plug it in here i...  \n",
      "1                          good case excellent value  \n",
      "2                              great for the jawbone  \n",
      "3  tied to charger for conversations lasting more...  \n",
      "4                                   the mic is great  \n"
     ]
    }
   ],
   "source": [
    "# ---------- Utility: Text cleaning ----------\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)           # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)              # remove mentions\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)    # remove special chars\n",
    "    return text.strip().lower()\n",
    "\n",
    "# ---------- TWCS Cleaning ----------\n",
    "df_twcs['tweet_id'] = pd.to_numeric(df_twcs['tweet_id'], errors='coerce')\n",
    "df_twcs['in_response_to_tweet_id'] = pd.to_numeric(df_twcs['in_response_to_tweet_id'], errors='coerce')\n",
    "\n",
    "# Parse created_at\n",
    "df_twcs['created_at'] = pd.to_datetime(\n",
    "    df_twcs['created_at'],\n",
    "    format=\"%Y-%m-%d %H:%M:%S%z\", \n",
    "    errors='coerce',\n",
    "    utc=True\n",
    ")\n",
    "\n",
    "# Split customer vs agent\n",
    "df_customer = df_twcs[df_twcs['inbound'] == True].copy()\n",
    "df_customer.rename(columns={\n",
    "    'tweet_id': 'customer_tweet_id',\n",
    "    'created_at': 'customer_created_at',\n",
    "    'text': 'customer_text'\n",
    "}, inplace=True)\n",
    "\n",
    "df_agent = df_twcs[df_twcs['inbound'] == False].copy()\n",
    "df_agent.rename(columns={'in_response_to_tweet_id': 'customer_tweet_id'}, inplace=True)\n",
    "\n",
    "# Merge on customer_tweet_id\n",
    "df_pairs = pd.merge(\n",
    "    df_agent,\n",
    "    df_customer[['customer_tweet_id', 'customer_created_at', 'customer_text']],\n",
    "    on='customer_tweet_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Calculate response time in minutes\n",
    "df_pairs['response_time'] = (\n",
    "    (df_pairs['created_at'] - df_pairs['customer_created_at']).dt.total_seconds() / 60\n",
    ")\n",
    "# Keep only valid response times\n",
    "df_pairs = df_pairs[df_pairs['response_time'] >= 0].copy()\n",
    "df_pairs['response_time'] = df_pairs['response_time'].clip(0, 1440)\n",
    "\n",
    "# Drop duplicates\n",
    "df_pairs = df_pairs.drop_duplicates(subset=['tweet_id'])\n",
    "\n",
    "# Create churn_label by keyword search in agent response\n",
    "churn_keywords = r'cancel|unhappy|disappointed|frustrated|bad|poor|terrible|awful|issue|problem|angry|upset|complain|worst|never|fail|horrible|pathetic|ridiculous|disaster|nightmare|sucks|furious'\n",
    "df_pairs['churn_label'] = df_pairs['text'].str.contains(churn_keywords, case=False, na=False).astype(int)\n",
    "\n",
    "# Clean text\n",
    "df_pairs['cleaned_text'] = df_pairs['text'].apply(clean_text)\n",
    "\n",
    "print(\"✅ Cleaned TWCS pairs shape:\", df_pairs.shape)\n",
    "print(df_pairs[['text', 'cleaned_text', 'response_time', 'churn_label']].head())\n",
    "\n",
    "# ---------- UCI Cleaning ----------\n",
    "df_uci['cleaned_text'] = df_uci['text'].apply(clean_text)\n",
    "\n",
    "print(\"\\n✅ Cleaned UCI preview:\")\n",
    "print(df_uci.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd3121e-9f02-42a5-a6c7-27397ecb24cb",
   "metadata": {},
   "source": [
    "## 💾 Step 5: Save Processed Data  \n",
    "\n",
    "We save the cleaned datasets into a `/data/processed/` folder for downstream tasks:\n",
    "\n",
    "- `cleaned_twcs.csv` → For churn modeling & summarization\n",
    "- `cleaned_uci.csv` → For validating the sentiment model\n",
    "\n",
    "👉 **Why?**  \n",
    "Keeping processed data separate from raw data ensures reproducibility and avoids overwriting raw sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a48c84d-650b-481e-8353-7d9234296d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed datasets saved.\n"
     ]
    }
   ],
   "source": [
    "df_pairs.to_csv('../data/processed/cleaned_twcs.csv', index=False)\n",
    "df_uci.to_csv('../data/processed/cleaned_uci.csv', index=False)\n",
    "\n",
    "print(\"✅ Processed datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef731591-c288-4d30-bdf0-363e8b58dd42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
